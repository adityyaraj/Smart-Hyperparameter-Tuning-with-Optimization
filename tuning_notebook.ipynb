{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport time\nimport copy\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\n# Define a simplified AlexNet for MNIST\nclass SimplifiedAlexNet(nn.Module):\n    def __init__(self, dropout_rate=0.5):\n        super(SimplifiedAlexNet, self).__init__()\n        # MNIST is 1x28x28, so we adapt AlexNet for smaller input\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=dropout_rate),\n            nn.Linear(128 * 7 * 7, 256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=dropout_rate),\n            nn.Linear(256, 10)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# Meta-Learner to remember past hyperparameter performance\nclass MetaLearner(nn.Module):\n    def __init__(self, input_size):\n        super(MetaLearner, self).__init__()\n        self.fc1 = nn.Linear(input_size, 32)\n        self.fc2 = nn.Linear(32, 1)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        return self.fc2(x)\n\n# Define Bayesian Multi-Armed Bandit for Hyperparameter Selection\n# The fix is in the BayesianBandit class\nclass BayesianBandit:\n    def __init__(self, param_space):\n        self.param_space = param_space\n        # Initialize history with string representation of tuples as keys\n        self.history = {}\n        for p in param_space:\n            self.history[str(p)] = []\n    \n    def sample(self, method=\"thompson\"):\n        \"\"\"Select hyperparameter using Bayesian Exploration\"\"\"\n        if method == \"thompson\":\n            sampled_means = []\n            for p in self.param_space:\n                # Use str(p) as the key for history\n                rewards = self.history.get(str(p), [])\n                if rewards:\n                    mean, std = np.mean(rewards), np.std(rewards) + 1e-4\n                else:\n                    mean, std = 0, 1\n                sampled_means.append(norm.rvs(mean, std))  # Thompson Sampling\n            return self.param_space[np.argmax(sampled_means)]\n        else:\n            return random.choice(self.param_space)\n    \n    def update(self, params, score):\n        \"\"\"Update the performance history\"\"\"\n        # Create the key if it doesn't exist\n        key = str(params)\n        if key not in self.history:\n            self.history[key] = []\n        self.history[key].append(score)\n# Genetic Algorithm for Hyperparameter Optimization\nclass GeneticOptimizer:\n    def __init__(self, param_bounds, population_size=10, mutation_rate=0.2, crossover_rate=0.7):\n        \"\"\"\n        param_bounds: Dictionary with parameter names as keys and (min, max, type) as values\n                     type can be 'float', 'int', or 'categorical' (for list of options)\n        \"\"\"\n        self.param_bounds = param_bounds\n        self.population_size = population_size\n        self.mutation_rate = mutation_rate\n        self.crossover_rate = crossover_rate\n        self.population = []\n        self.fitness_history = []\n        self.best_individual = None\n        self.best_fitness = float('-inf')\n        self.generation = 0\n        \n        # Initialize population\n        self._initialize_population()\n    \n    def _initialize_population(self):\n        \"\"\"Create initial random population\"\"\"\n        self.population = []\n        for _ in range(self.population_size):\n            individual = {}\n            for param, (min_val, max_val, param_type) in self.param_bounds.items():\n                if param_type == 'float':\n                    individual[param] = min_val + random.random() * (max_val - min_val)\n                elif param_type == 'int':\n                    individual[param] = random.randint(min_val, max_val)\n                elif param_type == 'categorical':\n                    individual[param] = random.choice(min_val)  # min_val is a list of options\n            self.population.append(individual)\n    \n    def _selection(self, fitness_scores):\n        \"\"\"Tournament selection\"\"\"\n        selected = []\n        for _ in range(self.population_size):\n            # Select 3 random individuals for tournament\n            indices = random.sample(range(self.population_size), 3)\n            tournament = [(i, fitness_scores[i]) for i in indices]\n            winner_idx = max(tournament, key=lambda x: x[1])[0]\n            selected.append(copy.deepcopy(self.population[winner_idx]))\n        return selected\n    \n    def _crossover(self, parent1, parent2):\n        \"\"\"Single point crossover between two parents\"\"\"\n        if random.random() > self.crossover_rate:\n            return copy.deepcopy(parent1)\n        \n        child = {}\n        # Determine crossover point\n        params = list(parent1.keys())\n        crossover_point = random.randint(0, len(params) - 1)\n        \n        for i, param in enumerate(params):\n            if i <= crossover_point:\n                child[param] = parent1[param]\n            else:\n                child[param] = parent2[param]\n        \n        return child\n    \n    def _mutate(self, individual):\n        \"\"\"Mutate individual with probability mutation_rate\"\"\"\n        mutated = copy.deepcopy(individual)\n        \n        for param, (min_val, max_val, param_type) in self.param_bounds.items():\n            # Apply mutation with probability mutation_rate\n            if random.random() < self.mutation_rate:\n                if param_type == 'float':\n                    # Gaussian mutation for float\n                    current = mutated[param]\n                    range_width = max_val - min_val\n                    mutation = random.gauss(0, range_width * 0.1)\n                    mutated[param] = max(min_val, min(max_val, current + mutation))\n                elif param_type == 'int':\n                    # Add or subtract by 1 (or more) for integers\n                    current = mutated[param]\n                    mutation = random.choice([-1, 1]) * random.randint(1, max(1, int((max_val - min_val) * 0.1)))\n                    mutated[param] = max(min_val, min(max_val, current + mutation))\n                elif param_type == 'categorical':\n                    # Random choice for categorical\n                    mutated[param] = random.choice(min_val)\n        \n        return mutated\n    \n    def evolve(self, fitness_scores):\n        \"\"\"Evolve the population based on fitness scores\"\"\"\n        # Update best individual\n        max_idx = np.argmax(fitness_scores)\n        if fitness_scores[max_idx] > self.best_fitness:\n            self.best_fitness = fitness_scores[max_idx]\n            self.best_individual = copy.deepcopy(self.population[max_idx])\n        \n        # Selection\n        selected = self._selection(fitness_scores)\n        \n        # Crossover and Mutation\n        new_population = []\n        for i in range(0, self.population_size, 2):\n            if i + 1 < self.population_size:\n                parent1, parent2 = selected[i], selected[i + 1]\n                child1 = self._crossover(parent1, parent2)\n                child2 = self._crossover(parent2, parent1)\n                \n                new_population.append(self._mutate(child1))\n                new_population.append(self._mutate(child2))\n            else:\n                # Handle odd population size\n                new_population.append(self._mutate(selected[i]))\n        \n        # Elitism: Replace worst individual with best from previous generation\n        if self.best_individual:\n            worst_idx = np.argmin(fitness_scores)\n            new_population[worst_idx] = copy.deepcopy(self.best_individual)\n        \n        self.population = new_population\n        self.generation += 1\n        \n        # Save fitness history\n        self.fitness_history.append(np.mean(fitness_scores))\n        \n        return self.population, self.best_individual, self.best_fitness\n    \n    def get_population_as_tuples(self, param_keys):\n        \"\"\"Convert population dictionary to tuples in specific order\"\"\"\n        return [tuple(ind[key] for key in param_keys) for ind in self.population]\n\n# Early Stopping Based on Gradient Flatness\nclass GradientEarlyStopping:\n    def __init__(self, patience=5, min_change=1e-3):\n        self.patience = patience\n        self.min_change = min_change\n        self.history = []\n    \n    def check_stop(self, loss):\n        \"\"\"Check if training should be stopped early\"\"\"\n        self.history.append(loss)\n        if len(self.history) > self.patience:\n            grad_changes = np.abs(np.diff(self.history[-self.patience:]))\n            if np.mean(grad_changes) < self.min_change:\n                return True\n        return False\n\n# Helper function to load MNIST data\ndef load_mnist_data(batch_size):\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n    \n    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n    test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    \n    return train_loader, test_loader\n\n# Train function for a single epoch\ndef train_epoch(model, device, train_loader, optimizer, criterion):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        \n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = output.max(1)\n        total += target.size(0)\n        correct += predicted.eq(target).sum().item()\n        \n        # Early batch limit for faster iterations during hyperparameter search\n        if batch_idx >= 50:  # Process only first 50 batches for quick evaluation\n            break\n    \n    return running_loss / (batch_idx + 1), 100. * correct / total\n\n# Test function for evaluation\ndef test(model, device, test_loader, criterion):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += criterion(output, target).item()\n            _, predicted = output.max(1)\n            total += target.size(0)\n            correct += predicted.eq(target).sum().item()\n    \n    return test_loss / len(test_loader), 100. * correct / total\n\n# Training function with hyperparameters\ndef train_model_with_params(params, max_epochs=5):\n    # Extract parameters\n    learning_rate, batch_size, dropout_rate, weight_decay = params\n    \n    # Setup device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Load data\n    train_loader, test_loader = load_mnist_data(int(batch_size))\n    \n    # Initialize model\n    model = SimplifiedAlexNet(dropout_rate=dropout_rate).to(device)\n    \n    # Setup optimizer and criterion\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    criterion = nn.CrossEntropyLoss()\n    \n    # Early stopping\n    early_stopper = GradientEarlyStopping(patience=3, min_change=1e-3)\n    \n    # Train\n    losses = []\n    for epoch in range(max_epochs):\n        train_loss, train_acc = train_epoch(model, device, train_loader, optimizer, criterion)\n        test_loss, test_acc = test(model, device, test_loader, criterion)\n        \n        losses.append(test_loss)\n        print(f\"Params: {params}, Epoch: {epoch+1}, Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%\")\n        \n        if early_stopper.check_stop(test_loss):\n            print(f\"Early stopping triggered at epoch {epoch+1}\")\n            break\n    \n    # Return negative loss as the score (higher is better)\n    return -np.mean(losses), test_acc\n\n# Main Smart Tuning Process with Hybrid Approach\ndef smart_hyperparameter_tuning(max_iters=30, genetic_gens=5):\n    \"\"\"\n    Performs Smart Hyperparameter Tuning combining Genetic Algorithm, \n    Bayesian Bandits and Meta-learning for AlexNet on MNIST\n    \"\"\"\n    print(\"Starting hybrid hyperparameter tuning with Genetic Algorithms, Bayesian Bandits, and Meta-learning\")\n    \n    # Define parameter bounds for genetic algorithm\n    param_bounds = {\n        'learning_rate': (0.0001, 0.01, 'float'),\n        'batch_size': (32, 256, 'int'),\n        'dropout_rate': (0.1, 0.7, 'float'),\n        'weight_decay': (0.00001, 0.001, 'float')\n    }\n    \n    # Initialize genetic optimizer\n    genetic_opt = GeneticOptimizer(\n        param_bounds=param_bounds,\n        population_size=10,\n        mutation_rate=0.2,\n        crossover_rate=0.7\n    )\n    \n    # Initialize meta-learner\n    meta_learner = MetaLearner(input_size=4)  # 4 hyperparameters\n    meta_optimizer = optim.Adam(meta_learner.parameters(), lr=0.01)\n    \n    # Track results\n    all_results = []\n    best_params, best_score, best_accuracy = None, float('-inf'), 0\n    \n    # Phase 1: Genetic Algorithm Exploration\n    print(\"\\n=== Phase 1: Genetic Algorithm Exploration ===\")\n    for gen in range(genetic_gens):\n        print(f\"\\n--- Generation {gen+1}/{genetic_gens} ---\")\n        \n        # Convert population to parameter tuples\n        param_keys = ['learning_rate', 'batch_size', 'dropout_rate', 'weight_decay']\n        population_tuples = genetic_opt.get_population_as_tuples(param_keys)\n        \n        # Evaluate population\n        fitness_scores = []\n        accuracies = []\n        \n        for i, params in enumerate(population_tuples):\n            print(f\"Individual {i+1}/{len(population_tuples)}: {params}\")\n            start_time = time.time()\n            score, accuracy = train_model_with_params(params)\n            end_time = time.time()\n            fitness_scores.append(score)\n            accuracies.append(accuracy)\n            \n            # Update best parameters\n            if score > best_score:\n                best_params, best_score, best_accuracy = params, score, accuracy\n            \n            # Track results\n            all_results.append({\n                'phase': 'genetic',\n                'generation': gen + 1,\n                'individual': i + 1,\n                'params': params,\n                'score': -score,  # Convert back to loss\n                'accuracy': accuracy,\n                'time': end_time - start_time\n            })\n            \n            # Update meta-learner\n            param_tensor = torch.tensor(params, dtype=torch.float32)\n            target_score = torch.tensor(score, dtype=torch.float32)\n            \n            meta_optimizer.zero_grad()\n            predicted_score = meta_learner(param_tensor)\n            meta_loss = F.mse_loss(predicted_score, target_score.view(-1, 1))\n            meta_loss.backward()\n            meta_optimizer.step()\n        \n        # Evolve population\n        genetic_opt.evolve(fitness_scores)\n        \n        print(f\"Generation {gen+1} best: {genetic_opt.best_individual}\")\n        print(f\"Generation {gen+1} best fitness: {genetic_opt.best_fitness:.4f}\")\n    \n    # Convert final genetic population to parameter space for Bayesian Bandit\n    final_population = genetic_opt.get_population_as_tuples(param_keys)\n    \n    # Add best individuals from each generation to the param space\n    param_space = list(set(final_population))\n    \n    # Phase 2: Bayesian Bandit Refinement\n    print(\"\\n=== Phase 2: Bayesian Bandit Refinement ===\")\n    bandit = BayesianBandit(param_space)\n    \n    # Initialize bandit with genetic results\n    for result in all_results:\n        if result['phase'] == 'genetic':\n            bandit.update(result['params'], -result['score'])  # Convert loss back to score\n    \n    remaining_iters = max_iters - genetic_gens * len(final_population)\n    for i in range(remaining_iters):\n        print(f\"\\n--- Iteration {i+1}/{remaining_iters} ---\")\n        \n        # Sample parameters using Bayesian Bandit\n        params = bandit.sample()\n        print(f\"Selected params: {params}\")\n        \n        # Use meta-learner to predict performance\n        param_tensor = torch.tensor(params, dtype=torch.float32)\n        with torch.no_grad():\n            predicted_score = meta_learner(param_tensor).item()\n        print(f\"Meta-learner predicted score: {predicted_score:.4f}\")\n        \n        # Train model\n        start_time = time.time()\n        score, accuracy = train_model_with_params(params)\n        end_time = time.time()\n        \n        # Update bandit\n        bandit.update(params, score)\n        \n        # Update meta-learner\n        target_score = torch.tensor(score, dtype=torch.float32)\n        meta_optimizer.zero_grad()\n        predicted_score = meta_learner(param_tensor)\n        meta_loss = F.mse_loss(predicted_score, target_score.view(-1, 1))\n        meta_loss.backward()\n        meta_optimizer.step()\n        \n        # Track results\n        all_results.append({\n            'phase': 'bandit',\n            'iteration': i + 1,\n            'params': params,\n            'score': -score,  # Convert back to loss\n            'accuracy': accuracy,\n            'time': end_time - start_time\n        })\n        \n        # Update best parameters\n        if score > best_score:\n            best_params, best_score, best_accuracy = params, score, accuracy\n        \n        print(f\"Current best params: {best_params} with accuracy: {best_accuracy:.2f}%\")\n    \n    # Plot results\n    plot_results(all_results)\n    \n    return best_params, best_score, best_accuracy\n\n# Helper function to plot results\ndef plot_results(results):\n    plt.figure(figsize=(15, 10))\n    \n    # Extract genetic results\n    genetic_results = [r for r in results if r['phase'] == 'genetic']\n    genetic_generations = [r['generation'] for r in genetic_results]\n    genetic_accuracies = [r['accuracy'] for r in genetic_results]\n    genetic_losses = [r['score'] for r in genetic_results]\n    \n    # Extract bandit results\n    bandit_results = [r for r in results if r['phase'] == 'bandit']\n    if bandit_results:\n        bandit_iterations = [r['iteration'] for r in bandit_results]\n        bandit_accuracies = [r['accuracy'] for r in bandit_results]\n        bandit_losses = [r['score'] for r in bandit_results]\n    \n    # Combined timeline for all evaluations\n    all_evals = list(range(1, len(results) + 1))\n    all_accuracies = [r['accuracy'] for r in results]\n    all_losses = [r['score'] for r in results]\n    \n    # Plot overall progress\n    plt.subplot(3, 2, 1)\n    plt.plot(all_evals, all_accuracies, 'bo-')\n    plt.title('Overall Accuracy vs Evaluation')\n    plt.xlabel('Evaluation Number')\n    plt.ylabel('Accuracy (%)')\n    plt.grid(True)\n    \n    plt.subplot(3, 2, 2)\n    plt.plot(all_evals, all_losses, 'ro-')\n    plt.title('Overall Loss vs Evaluation')\n    plt.xlabel('Evaluation Number')\n    plt.ylabel('Loss')\n    plt.grid(True)\n    \n    # Plot genetic algorithm results\n    plt.subplot(3, 2, 3)\n    plt.scatter(genetic_generations, genetic_accuracies)\n    plt.title('Genetic Algorithm: Accuracy vs Generation')\n    plt.xlabel('Generation')\n    plt.ylabel('Accuracy (%)')\n    plt.grid(True)\n    \n    plt.subplot(3, 2, 4)\n    plt.scatter(genetic_generations, genetic_losses)\n    plt.title('Genetic Algorithm: Loss vs Generation')\n    plt.xlabel('Generation')\n    plt.ylabel('Loss')\n    plt.grid(True)\n    \n    # Plot bandit results if available\n    if bandit_results:\n        plt.subplot(3, 2, 5)\n        plt.plot(bandit_iterations, bandit_accuracies, 'go-')\n        plt.title('Bandit: Accuracy vs Iteration')\n        plt.xlabel('Iteration')\n        plt.ylabel('Accuracy (%)')\n        plt.grid(True)\n        \n        plt.subplot(3, 2, 6)\n        plt.plot(bandit_iterations, bandit_losses, 'mo-')\n        plt.title('Bandit: Loss vs Iteration')\n        plt.xlabel('Iteration')\n        plt.ylabel('Loss')\n        plt.grid(True)\n    \n    plt.tight_layout()\n    plt.savefig('hybrid_tuning_results.png')\n    plt.close()\n\nif __name__ == \"__main__\":\n    print(\"Starting Hybrid Hyperparameter Tuning for AlexNet on MNIST\")\n    best_params, best_score, best_accuracy = smart_hyperparameter_tuning(max_iters=25, genetic_gens=3)\n    \n    print(\"\\n=== Final Results ===\")\n    print(f\"Best Hyperparameters: {best_params}\")\n    print(f\"Best Score: {-best_score:.4f}\")\n    print(f\"Best Accuracy: {best_accuracy:.2f}%\")\n    \n    # Train a final model with the best parameters\n    print(\"\\nTraining final model with best parameters...\")\n    final_score, final_accuracy = train_model_with_params(best_params, max_epochs=10)\n    \n    print(f\"Final Model Performance - Loss: {-final_score:.4f}, Accuracy: {final_accuracy:.2f}%\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-12T11:48:20.212470Z","iopub.execute_input":"2025-04-12T11:48:20.212937Z","iopub.status.idle":"2025-04-12T11:59:04.548138Z","shell.execute_reply.started":"2025-04-12T11:48:20.212913Z","shell.execute_reply":"2025-04-12T11:59:04.547229Z"}},"outputs":[{"name":"stdout","text":"Starting Hybrid Hyperparameter Tuning for AlexNet on MNIST\nStarting hybrid hyperparameter tuning with Genetic Algorithms, Bayesian Bandits, and Meta-learning\n\n=== Phase 1: Genetic Algorithm Exploration ===\n\n--- Generation 1/3 ---\nIndividual 1/10: (0.00643032530473305, 38, 0.5449302998558997, 0.00025244293526544145)\nParams: (0.00643032530473305, 38, 0.5449302998558997, 0.00025244293526544145), Epoch: 1, Train Acc: 41.43%, Test Acc: 83.32%\nParams: (0.00643032530473305, 38, 0.5449302998558997, 0.00025244293526544145), Epoch: 2, Train Acc: 81.11%, Test Acc: 91.82%\nParams: (0.00643032530473305, 38, 0.5449302998558997, 0.00025244293526544145), Epoch: 3, Train Acc: 86.48%, Test Acc: 94.03%\nParams: (0.00643032530473305, 38, 0.5449302998558997, 0.00025244293526544145), Epoch: 4, Train Acc: 88.18%, Test Acc: 92.31%\nParams: (0.00643032530473305, 38, 0.5449302998558997, 0.00025244293526544145), Epoch: 5, Train Acc: 89.16%, Test Acc: 94.75%\nIndividual 2/10: (0.0014814254923989248, 58, 0.5060196924537468, 0.000893257772027797)\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-2-5f50812609f1>:409: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  meta_loss = F.mse_loss(predicted_score, target_score.view(-1, 1))\n","output_type":"stream"},{"name":"stdout","text":"Params: (0.0014814254923989248, 58, 0.5060196924537468, 0.000893257772027797), Epoch: 1, Train Acc: 70.69%, Test Acc: 90.93%\nParams: (0.0014814254923989248, 58, 0.5060196924537468, 0.000893257772027797), Epoch: 2, Train Acc: 92.12%, Test Acc: 96.21%\nParams: (0.0014814254923989248, 58, 0.5060196924537468, 0.000893257772027797), Epoch: 3, Train Acc: 93.37%, Test Acc: 96.79%\nParams: (0.0014814254923989248, 58, 0.5060196924537468, 0.000893257772027797), Epoch: 4, Train Acc: 94.96%, Test Acc: 96.76%\nParams: (0.0014814254923989248, 58, 0.5060196924537468, 0.000893257772027797), Epoch: 5, Train Acc: 95.40%, Test Acc: 97.99%\nIndividual 3/10: (0.00096069444303122, 140, 0.11906960768907016, 0.00010275828746297653)\nParams: (0.00096069444303122, 140, 0.11906960768907016, 0.00010275828746297653), Epoch: 1, Train Acc: 82.20%, Test Acc: 95.20%\nParams: (0.00096069444303122, 140, 0.11906960768907016, 0.00010275828746297653), Epoch: 2, Train Acc: 95.70%, Test Acc: 97.31%\nParams: (0.00096069444303122, 140, 0.11906960768907016, 0.00010275828746297653), Epoch: 3, Train Acc: 97.07%, Test Acc: 97.22%\nParams: (0.00096069444303122, 140, 0.11906960768907016, 0.00010275828746297653), Epoch: 4, Train Acc: 97.16%, Test Acc: 97.61%\nParams: (0.00096069444303122, 140, 0.11906960768907016, 0.00010275828746297653), Epoch: 5, Train Acc: 97.86%, Test Acc: 98.29%\nIndividual 4/10: (0.002403342844568322, 186, 0.11592158181031818, 0.000206849274179782)\nParams: (0.002403342844568322, 186, 0.11592158181031818, 0.000206849274179782), Epoch: 1, Train Acc: 83.99%, Test Acc: 96.48%\nParams: (0.002403342844568322, 186, 0.11592158181031818, 0.000206849274179782), Epoch: 2, Train Acc: 96.90%, Test Acc: 97.79%\nParams: (0.002403342844568322, 186, 0.11592158181031818, 0.000206849274179782), Epoch: 3, Train Acc: 97.32%, Test Acc: 97.87%\nParams: (0.002403342844568322, 186, 0.11592158181031818, 0.000206849274179782), Epoch: 4, Train Acc: 98.03%, Test Acc: 98.37%\nParams: (0.002403342844568322, 186, 0.11592158181031818, 0.000206849274179782), Epoch: 5, Train Acc: 97.90%, Test Acc: 97.82%\nIndividual 5/10: (0.006533855934017281, 171, 0.3517118925769952, 0.0004547169558210151)\nParams: (0.006533855934017281, 171, 0.3517118925769952, 0.0004547169558210151), Epoch: 1, Train Acc: 74.53%, Test Acc: 93.29%\nParams: (0.006533855934017281, 171, 0.3517118925769952, 0.0004547169558210151), Epoch: 2, Train Acc: 92.41%, Test Acc: 95.73%\nParams: (0.006533855934017281, 171, 0.3517118925769952, 0.0004547169558210151), Epoch: 3, Train Acc: 94.74%, Test Acc: 96.59%\nParams: (0.006533855934017281, 171, 0.3517118925769952, 0.0004547169558210151), Epoch: 4, Train Acc: 95.21%, Test Acc: 97.13%\nParams: (0.006533855934017281, 171, 0.3517118925769952, 0.0004547169558210151), Epoch: 5, Train Acc: 95.83%, Test Acc: 96.95%\nIndividual 6/10: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798)\nParams: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 1, Train Acc: 84.11%, Test Acc: 96.41%\nParams: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 2, Train Acc: 97.24%, Test Acc: 97.69%\nParams: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 3, Train Acc: 97.54%, Test Acc: 98.13%\nParams: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 4, Train Acc: 97.60%, Test Acc: 98.03%\nParams: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 5, Train Acc: 98.08%, Test Acc: 98.12%\nIndividual 7/10: (0.007011580010383447, 119, 0.2667228050029851, 0.0002231606244865129)\nParams: (0.007011580010383447, 119, 0.2667228050029851, 0.0002231606244865129), Epoch: 1, Train Acc: 68.58%, Test Acc: 91.07%\nParams: (0.007011580010383447, 119, 0.2667228050029851, 0.0002231606244865129), Epoch: 2, Train Acc: 92.59%, Test Acc: 94.89%\nParams: (0.007011580010383447, 119, 0.2667228050029851, 0.0002231606244865129), Epoch: 3, Train Acc: 93.89%, Test Acc: 96.40%\nParams: (0.007011580010383447, 119, 0.2667228050029851, 0.0002231606244865129), Epoch: 4, Train Acc: 95.16%, Test Acc: 96.78%\nParams: (0.007011580010383447, 119, 0.2667228050029851, 0.0002231606244865129), Epoch: 5, Train Acc: 95.53%, Test Acc: 97.25%\nIndividual 8/10: (0.007658591877164588, 58, 0.15564750602808874, 0.00010574921306512938)\nParams: (0.007658591877164588, 58, 0.15564750602808874, 0.00010574921306512938), Epoch: 1, Train Acc: 54.06%, Test Acc: 86.14%\nParams: (0.007658591877164588, 58, 0.15564750602808874, 0.00010574921306512938), Epoch: 2, Train Acc: 86.85%, Test Acc: 90.79%\nParams: (0.007658591877164588, 58, 0.15564750602808874, 0.00010574921306512938), Epoch: 3, Train Acc: 88.74%, Test Acc: 92.44%\nParams: (0.007658591877164588, 58, 0.15564750602808874, 0.00010574921306512938), Epoch: 4, Train Acc: 89.76%, Test Acc: 90.26%\nParams: (0.007658591877164588, 58, 0.15564750602808874, 0.00010574921306512938), Epoch: 5, Train Acc: 92.09%, Test Acc: 93.93%\nIndividual 9/10: (0.008490194226839852, 186, 0.25871252033320785, 5.301593849386622e-05)\nParams: (0.008490194226839852, 186, 0.25871252033320785, 5.301593849386622e-05), Epoch: 1, Train Acc: 62.60%, Test Acc: 90.13%\nParams: (0.008490194226839852, 186, 0.25871252033320785, 5.301593849386622e-05), Epoch: 2, Train Acc: 90.11%, Test Acc: 93.24%\nParams: (0.008490194226839852, 186, 0.25871252033320785, 5.301593849386622e-05), Epoch: 3, Train Acc: 92.52%, Test Acc: 93.94%\nParams: (0.008490194226839852, 186, 0.25871252033320785, 5.301593849386622e-05), Epoch: 4, Train Acc: 92.40%, Test Acc: 94.63%\nParams: (0.008490194226839852, 186, 0.25871252033320785, 5.301593849386622e-05), Epoch: 5, Train Acc: 93.50%, Test Acc: 96.12%\nIndividual 10/10: (0.004648306308751242, 63, 0.6838694583876224, 0.00038474903343626996)\nParams: (0.004648306308751242, 63, 0.6838694583876224, 0.00038474903343626996), Epoch: 1, Train Acc: 62.25%, Test Acc: 92.69%\nParams: (0.004648306308751242, 63, 0.6838694583876224, 0.00038474903343626996), Epoch: 2, Train Acc: 86.46%, Test Acc: 96.07%\nParams: (0.004648306308751242, 63, 0.6838694583876224, 0.00038474903343626996), Epoch: 3, Train Acc: 88.64%, Test Acc: 94.15%\nParams: (0.004648306308751242, 63, 0.6838694583876224, 0.00038474903343626996), Epoch: 4, Train Acc: 90.91%, Test Acc: 96.39%\nParams: (0.004648306308751242, 63, 0.6838694583876224, 0.00038474903343626996), Epoch: 5, Train Acc: 92.90%, Test Acc: 96.95%\nGeneration 1 best: {'learning_rate': 0.002854088011483561, 'batch_size': 254, 'dropout_rate': 0.10389925580683662, 'weight_decay': 0.0008077610593144798}\nGeneration 1 best fitness: -0.0713\n\n--- Generation 2/3 ---\nIndividual 1/10: (0.002854088011483561, 186, 0.18229071431333144, 0.000206849274179782)\nParams: (0.002854088011483561, 186, 0.18229071431333144, 0.000206849274179782), Epoch: 1, Train Acc: 81.96%, Test Acc: 96.25%\nParams: (0.002854088011483561, 186, 0.18229071431333144, 0.000206849274179782), Epoch: 2, Train Acc: 96.37%, Test Acc: 97.87%\nParams: (0.002854088011483561, 186, 0.18229071431333144, 0.000206849274179782), Epoch: 3, Train Acc: 97.23%, Test Acc: 97.42%\nParams: (0.002854088011483561, 186, 0.18229071431333144, 0.000206849274179782), Epoch: 4, Train Acc: 97.62%, Test Acc: 98.34%\nParams: (0.002854088011483561, 186, 0.18229071431333144, 0.000206849274179782), Epoch: 5, Train Acc: 97.82%, Test Acc: 98.66%\nIndividual 2/10: (0.002403342844568322, 254, 0.10389925580683662, 0.0008077610593144798)\nParams: (0.002403342844568322, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 1, Train Acc: 75.91%, Test Acc: 94.27%\nParams: (0.002403342844568322, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 2, Train Acc: 95.94%, Test Acc: 97.62%\nParams: (0.002403342844568322, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 3, Train Acc: 97.34%, Test Acc: 97.89%\nParams: (0.002403342844568322, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 4, Train Acc: 97.31%, Test Acc: 98.22%\nParams: (0.002403342844568322, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 5, Train Acc: 97.90%, Test Acc: 98.52%\nIndividual 3/10: (0.002403342844568322, 186, 0.11592158181031818, 0.000206849274179782)\nParams: (0.002403342844568322, 186, 0.11592158181031818, 0.000206849274179782), Epoch: 1, Train Acc: 81.11%, Test Acc: 96.33%\nParams: (0.002403342844568322, 186, 0.11592158181031818, 0.000206849274179782), Epoch: 2, Train Acc: 96.54%, Test Acc: 97.84%\nParams: (0.002403342844568322, 186, 0.11592158181031818, 0.000206849274179782), Epoch: 3, Train Acc: 97.22%, Test Acc: 97.53%\nParams: (0.002403342844568322, 186, 0.11592158181031818, 0.000206849274179782), Epoch: 4, Train Acc: 97.88%, Test Acc: 98.62%\nParams: (0.002403342844568322, 186, 0.11592158181031818, 0.000206849274179782), Epoch: 5, Train Acc: 98.26%, Test Acc: 98.38%\nIndividual 4/10: (0.002403342844568322, 189, 0.11592158181031818, 0.000206849274179782)\nParams: (0.002403342844568322, 189, 0.11592158181031818, 0.000206849274179782), Epoch: 1, Train Acc: 83.08%, Test Acc: 96.16%\nParams: (0.002403342844568322, 189, 0.11592158181031818, 0.000206849274179782), Epoch: 2, Train Acc: 96.62%, Test Acc: 97.86%\nParams: (0.002403342844568322, 189, 0.11592158181031818, 0.000206849274179782), Epoch: 3, Train Acc: 97.68%, Test Acc: 98.42%\nParams: (0.002403342844568322, 189, 0.11592158181031818, 0.000206849274179782), Epoch: 4, Train Acc: 97.96%, Test Acc: 98.25%\nParams: (0.002403342844568322, 189, 0.11592158181031818, 0.000206849274179782), Epoch: 5, Train Acc: 98.30%, Test Acc: 98.71%\nIndividual 5/10: (0.006533855934017281, 191, 0.10389925580683662, 0.0008077610593144798)\nParams: (0.006533855934017281, 191, 0.10389925580683662, 0.0008077610593144798), Epoch: 1, Train Acc: 75.22%, Test Acc: 94.59%\nParams: (0.006533855934017281, 191, 0.10389925580683662, 0.0008077610593144798), Epoch: 2, Train Acc: 95.42%, Test Acc: 96.60%\nParams: (0.006533855934017281, 191, 0.10389925580683662, 0.0008077610593144798), Epoch: 3, Train Acc: 96.09%, Test Acc: 96.77%\nParams: (0.006533855934017281, 191, 0.10389925580683662, 0.0008077610593144798), Epoch: 4, Train Acc: 96.74%, Test Acc: 97.02%\nParams: (0.006533855934017281, 191, 0.10389925580683662, 0.0008077610593144798), Epoch: 5, Train Acc: 96.74%, Test Acc: 97.36%\nIndividual 6/10: (0.002854088011483561, 254, 0.1, 0.0008077610593144798)\nParams: (0.002854088011483561, 254, 0.1, 0.0008077610593144798), Epoch: 1, Train Acc: 80.86%, Test Acc: 96.43%\nParams: (0.002854088011483561, 254, 0.1, 0.0008077610593144798), Epoch: 2, Train Acc: 96.31%, Test Acc: 98.04%\nParams: (0.002854088011483561, 254, 0.1, 0.0008077610593144798), Epoch: 3, Train Acc: 97.38%, Test Acc: 98.39%\nParams: (0.002854088011483561, 254, 0.1, 0.0008077610593144798), Epoch: 4, Train Acc: 97.92%, Test Acc: 98.12%\nParams: (0.002854088011483561, 254, 0.1, 0.0008077610593144798), Epoch: 5, Train Acc: 98.01%, Test Acc: 98.49%\nIndividual 7/10: (0.002854088011483561, 254, 0.19989674497553284, 0.000206849274179782)\nParams: (0.002854088011483561, 254, 0.19989674497553284, 0.000206849274179782), Epoch: 1, Train Acc: 82.16%, Test Acc: 96.77%\nParams: (0.002854088011483561, 254, 0.19989674497553284, 0.000206849274179782), Epoch: 2, Train Acc: 96.68%, Test Acc: 97.93%\nParams: (0.002854088011483561, 254, 0.19989674497553284, 0.000206849274179782), Epoch: 3, Train Acc: 97.00%, Test Acc: 98.34%\nParams: (0.002854088011483561, 254, 0.19989674497553284, 0.000206849274179782), Epoch: 4, Train Acc: 97.88%, Test Acc: 98.30%\nParams: (0.002854088011483561, 254, 0.19989674497553284, 0.000206849274179782), Epoch: 5, Train Acc: 98.20%, Test Acc: 97.85%\nIndividual 8/10: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798)\nParams: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 1, Train Acc: 83.63%, Test Acc: 96.97%\nParams: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 2, Train Acc: 96.95%, Test Acc: 97.39%\nParams: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 3, Train Acc: 97.48%, Test Acc: 97.95%\nParams: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 4, Train Acc: 98.12%, Test Acc: 98.51%\nParams: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 5, Train Acc: 98.01%, Test Acc: 98.58%\nIndividual 9/10: (0.00096069444303122, 140, 0.11592158181031818, 0.00019802787262407704)\nParams: (0.00096069444303122, 140, 0.11592158181031818, 0.00019802787262407704), Epoch: 1, Train Acc: 79.48%, Test Acc: 93.59%\nParams: (0.00096069444303122, 140, 0.11592158181031818, 0.00019802787262407704), Epoch: 2, Train Acc: 95.28%, Test Acc: 96.29%\nParams: (0.00096069444303122, 140, 0.11592158181031818, 0.00019802787262407704), Epoch: 3, Train Acc: 96.23%, Test Acc: 97.91%\nParams: (0.00096069444303122, 140, 0.11592158181031818, 0.00019802787262407704), Epoch: 4, Train Acc: 97.63%, Test Acc: 98.01%\nParams: (0.00096069444303122, 140, 0.11592158181031818, 0.00019802787262407704), Epoch: 5, Train Acc: 97.73%, Test Acc: 98.38%\nIndividual 10/10: (0.002403342844568322, 186, 0.11906960768907016, 0.00010275828746297653)\nParams: (0.002403342844568322, 186, 0.11906960768907016, 0.00010275828746297653), Epoch: 1, Train Acc: 84.77%, Test Acc: 96.24%\nParams: (0.002403342844568322, 186, 0.11906960768907016, 0.00010275828746297653), Epoch: 2, Train Acc: 96.57%, Test Acc: 97.21%\nParams: (0.002403342844568322, 186, 0.11906960768907016, 0.00010275828746297653), Epoch: 3, Train Acc: 97.72%, Test Acc: 97.88%\nParams: (0.002403342844568322, 186, 0.11906960768907016, 0.00010275828746297653), Epoch: 4, Train Acc: 98.10%, Test Acc: 98.64%\nParams: (0.002403342844568322, 186, 0.11906960768907016, 0.00010275828746297653), Epoch: 5, Train Acc: 98.09%, Test Acc: 98.62%\nGeneration 2 best: {'learning_rate': 0.002854088011483561, 'batch_size': 254, 'dropout_rate': 0.10389925580683662, 'weight_decay': 0.0008077610593144798}\nGeneration 2 best fitness: -0.0656\n\n--- Generation 3/3 ---\nIndividual 1/10: (0.0025787660233185755, 189, 0.10389925580683662, 0.0008077610593144798)\nParams: (0.0025787660233185755, 189, 0.10389925580683662, 0.0008077610593144798), Epoch: 1, Train Acc: 80.10%, Test Acc: 96.34%\nParams: (0.0025787660233185755, 189, 0.10389925580683662, 0.0008077610593144798), Epoch: 2, Train Acc: 96.58%, Test Acc: 97.52%\nParams: (0.0025787660233185755, 189, 0.10389925580683662, 0.0008077610593144798), Epoch: 3, Train Acc: 97.26%, Test Acc: 97.78%\nParams: (0.0025787660233185755, 189, 0.10389925580683662, 0.0008077610593144798), Epoch: 4, Train Acc: 97.48%, Test Acc: 97.98%\nParams: (0.0025787660233185755, 189, 0.10389925580683662, 0.0008077610593144798), Epoch: 5, Train Acc: 97.88%, Test Acc: 98.13%\nIndividual 2/10: (0.002854088011483561, 180, 0.11592158181031818, 0.000206849274179782)\nParams: (0.002854088011483561, 180, 0.11592158181031818, 0.000206849274179782), Epoch: 1, Train Acc: 85.14%, Test Acc: 96.41%\nParams: (0.002854088011483561, 180, 0.11592158181031818, 0.000206849274179782), Epoch: 2, Train Acc: 97.10%, Test Acc: 98.27%\nParams: (0.002854088011483561, 180, 0.11592158181031818, 0.000206849274179782), Epoch: 3, Train Acc: 97.52%, Test Acc: 98.57%\nParams: (0.002854088011483561, 180, 0.11592158181031818, 0.000206849274179782), Epoch: 4, Train Acc: 98.06%, Test Acc: 98.35%\nParams: (0.002854088011483561, 180, 0.11592158181031818, 0.000206849274179782), Epoch: 5, Train Acc: 98.10%, Test Acc: 98.44%\nIndividual 3/10: (0.002403342844568322, 164, 0.11906960768907016, 0.00010275828746297653)\nParams: (0.002403342844568322, 164, 0.11906960768907016, 0.00010275828746297653), Epoch: 1, Train Acc: 81.65%, Test Acc: 95.83%\nParams: (0.002403342844568322, 164, 0.11906960768907016, 0.00010275828746297653), Epoch: 2, Train Acc: 96.88%, Test Acc: 97.11%\nParams: (0.002403342844568322, 164, 0.11906960768907016, 0.00010275828746297653), Epoch: 3, Train Acc: 97.19%, Test Acc: 98.56%\nParams: (0.002403342844568322, 164, 0.11906960768907016, 0.00010275828746297653), Epoch: 4, Train Acc: 97.72%, Test Acc: 98.62%\nParams: (0.002403342844568322, 164, 0.11906960768907016, 0.00010275828746297653), Epoch: 5, Train Acc: 98.45%, Test Acc: 98.53%\nIndividual 4/10: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798)\nParams: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 1, Train Acc: 82.04%, Test Acc: 96.17%\nParams: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 2, Train Acc: 96.66%, Test Acc: 97.94%\nParams: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 3, Train Acc: 97.27%, Test Acc: 97.58%\nParams: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 4, Train Acc: 97.61%, Test Acc: 98.41%\nParams: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 5, Train Acc: 98.05%, Test Acc: 98.02%\nIndividual 5/10: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798)\nParams: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 1, Train Acc: 83.52%, Test Acc: 96.80%\nParams: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 2, Train Acc: 96.80%, Test Acc: 97.93%\nParams: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 3, Train Acc: 97.50%, Test Acc: 97.96%\nParams: (0.002854088011483561, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 4, Train Acc: 97.96%, Test Acc: 97.88%\nEarly stopping triggered at epoch 4\nIndividual 6/10: (0.002403342844568322, 187, 0.11592158181031818, 0.000206849274179782)\nParams: (0.002403342844568322, 187, 0.11592158181031818, 0.000206849274179782), Epoch: 1, Train Acc: 85.40%, Test Acc: 96.51%\nParams: (0.002403342844568322, 187, 0.11592158181031818, 0.000206849274179782), Epoch: 2, Train Acc: 96.47%, Test Acc: 97.85%\nParams: (0.002403342844568322, 187, 0.11592158181031818, 0.000206849274179782), Epoch: 3, Train Acc: 97.63%, Test Acc: 97.40%\nParams: (0.002403342844568322, 187, 0.11592158181031818, 0.000206849274179782), Epoch: 4, Train Acc: 97.76%, Test Acc: 97.76%\nParams: (0.002403342844568322, 187, 0.11592158181031818, 0.000206849274179782), Epoch: 5, Train Acc: 98.07%, Test Acc: 98.57%\nIndividual 7/10: (0.002854088011483561, 256, 0.10389925580683662, 0.0008077610593144798)\nParams: (0.002854088011483561, 256, 0.10389925580683662, 0.0008077610593144798), Epoch: 1, Train Acc: 83.33%, Test Acc: 96.71%\nParams: (0.002854088011483561, 256, 0.10389925580683662, 0.0008077610593144798), Epoch: 2, Train Acc: 96.79%, Test Acc: 97.19%\nParams: (0.002854088011483561, 256, 0.10389925580683662, 0.0008077610593144798), Epoch: 3, Train Acc: 97.23%, Test Acc: 98.09%\nParams: (0.002854088011483561, 256, 0.10389925580683662, 0.0008077610593144798), Epoch: 4, Train Acc: 97.79%, Test Acc: 98.71%\nParams: (0.002854088011483561, 256, 0.10389925580683662, 0.0008077610593144798), Epoch: 5, Train Acc: 98.17%, Test Acc: 98.44%\nIndividual 8/10: (0.0038624965697843993, 254, 0.10389925580683662, 0.0008077610593144798)\nParams: (0.0038624965697843993, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 1, Train Acc: 75.29%, Test Acc: 95.48%\nParams: (0.0038624965697843993, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 2, Train Acc: 95.74%, Test Acc: 97.21%\nParams: (0.0038624965697843993, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 3, Train Acc: 96.87%, Test Acc: 97.48%\nParams: (0.0038624965697843993, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 4, Train Acc: 97.06%, Test Acc: 97.32%\nParams: (0.0038624965697843993, 254, 0.10389925580683662, 0.0008077610593144798), Epoch: 5, Train Acc: 97.65%, Test Acc: 97.95%\nIndividual 9/10: (0.002403342844568322, 189, 0.11592158181031818, 0.000206849274179782)\nParams: (0.002403342844568322, 189, 0.11592158181031818, 0.000206849274179782), Epoch: 1, Train Acc: 80.66%, Test Acc: 96.50%\nParams: (0.002403342844568322, 189, 0.11592158181031818, 0.000206849274179782), Epoch: 2, Train Acc: 97.03%, Test Acc: 98.18%\nParams: (0.002403342844568322, 189, 0.11592158181031818, 0.000206849274179782), Epoch: 3, Train Acc: 97.08%, Test Acc: 98.49%\nParams: (0.002403342844568322, 189, 0.11592158181031818, 0.000206849274179782), Epoch: 4, Train Acc: 98.02%, Test Acc: 98.14%\nParams: (0.002403342844568322, 189, 0.11592158181031818, 0.000206849274179782), Epoch: 5, Train Acc: 98.16%, Test Acc: 98.38%\nIndividual 10/10: (0.002854088011483561, 252, 0.1, 0.0008077610593144798)\nParams: (0.002854088011483561, 252, 0.1, 0.0008077610593144798), Epoch: 1, Train Acc: 82.77%, Test Acc: 96.50%\nParams: (0.002854088011483561, 252, 0.1, 0.0008077610593144798), Epoch: 2, Train Acc: 96.74%, Test Acc: 97.19%\nParams: (0.002854088011483561, 252, 0.1, 0.0008077610593144798), Epoch: 3, Train Acc: 97.44%, Test Acc: 98.44%\nParams: (0.002854088011483561, 252, 0.1, 0.0008077610593144798), Epoch: 4, Train Acc: 97.72%, Test Acc: 98.34%\nParams: (0.002854088011483561, 252, 0.1, 0.0008077610593144798), Epoch: 5, Train Acc: 97.84%, Test Acc: 98.50%\nGeneration 3 best: {'learning_rate': 0.002854088011483561, 'batch_size': 180, 'dropout_rate': 0.11592158181031818, 'weight_decay': 0.000206849274179782}\nGeneration 3 best fitness: -0.0626\n\n=== Phase 2: Bayesian Bandit Refinement ===\n\n=== Final Results ===\nBest Hyperparameters: (0.002854088011483561, 180, 0.11592158181031818, 0.000206849274179782)\nBest Score: 0.0626\nBest Accuracy: 98.44%\n\nTraining final model with best parameters...\nParams: (0.002854088011483561, 180, 0.11592158181031818, 0.000206849274179782), Epoch: 1, Train Acc: 84.16%, Test Acc: 96.76%\nParams: (0.002854088011483561, 180, 0.11592158181031818, 0.000206849274179782), Epoch: 2, Train Acc: 97.16%, Test Acc: 97.87%\nParams: (0.002854088011483561, 180, 0.11592158181031818, 0.000206849274179782), Epoch: 3, Train Acc: 97.25%, Test Acc: 97.59%\nParams: (0.002854088011483561, 180, 0.11592158181031818, 0.000206849274179782), Epoch: 4, Train Acc: 97.93%, Test Acc: 98.44%\nParams: (0.002854088011483561, 180, 0.11592158181031818, 0.000206849274179782), Epoch: 5, Train Acc: 97.93%, Test Acc: 98.23%\nParams: (0.002854088011483561, 180, 0.11592158181031818, 0.000206849274179782), Epoch: 6, Train Acc: 98.26%, Test Acc: 98.56%\nParams: (0.002854088011483561, 180, 0.11592158181031818, 0.000206849274179782), Epoch: 7, Train Acc: 98.41%, Test Acc: 98.43%\nParams: (0.002854088011483561, 180, 0.11592158181031818, 0.000206849274179782), Epoch: 8, Train Acc: 98.36%, Test Acc: 98.66%\nParams: (0.002854088011483561, 180, 0.11592158181031818, 0.000206849274179782), Epoch: 9, Train Acc: 98.42%, Test Acc: 98.39%\nParams: (0.002854088011483561, 180, 0.11592158181031818, 0.000206849274179782), Epoch: 10, Train Acc: 98.69%, Test Acc: 98.86%\nFinal Model Performance - Loss: 0.0560, Accuracy: 98.86%\n","output_type":"stream"}],"execution_count":2}]}